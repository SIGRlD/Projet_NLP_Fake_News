{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exemple de pré-traitement des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gensim.models import KeyedVectors\n",
    "from donnees.nettoyage import load_dataset, clean_dataset, add_columns\n",
    "from donnees.utils import FakeNewsDataset, ajuster_canaux\n",
    "from embedding import GloVeModel, tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Nettoyage*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne pas oublier d'importer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement\n",
    "data_train = load_dataset(\"./donnees/Task3_english_training.csv\")\n",
    "data_train = clean_dataset(data_train)\n",
    "data_train = add_columns(data_train)\n",
    "# Validation\n",
    "data_dev = load_dataset(\"./donnees/Task3_english_dev.csv\")\n",
    "data_dev = clean_dataset(data_dev)\n",
    "data_dev = add_columns(data_dev)\n",
    "# Test\n",
    "data_test = load_dataset(\"./donnees/English_data_test_release_with_rating.csv\")\n",
    "data_test = clean_dataset(data_test)\n",
    "data_test = add_columns(data_test)\n",
    "print(f\"Entrainement : {data_train.shape[0]} | Validation : {data_dev.shape[0]} | Test : {data_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne pas oublier d'importer les embeddings pré-entrainés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir glove ou word2Vec\n",
    "glove = GloVeModel(\"./donnees/glove.6B/glove.6B.100d.txt\")\n",
    "# word2Vec = KeyedVectors.load_word2vec_format(\"./donnees/GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetoniser les donnees d'entrainement\n",
    "# Si pad=True, retourne un tenseur, sinon retourne une liste\n",
    "tokens_train = tokeniser(data_train.full_text, modele=glove, pad=False)\n",
    "if isinstance(tokens_train,torch.Tensor):\n",
    "    print(tokens_train.shape)\n",
    "else:\n",
    "    print(len(tokens_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cibles d'entrainement\n",
    "cible_train_real = torch.tensor(data_train[\"true\"],dtype=torch.float32)\n",
    "# cible_train_fake = torch.tensor(data_train[\"false\"],dtype=torch.float32)\n",
    "# cible_train_part = torch.tensor(data_train[\"partially_false\"],dtype=torch.float32)\n",
    "# cible_train_oth = torch.tensor(data_train[\"other\"],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetoniser les donnees de validation\n",
    "# Si pad=True, retourne un tenseur, sinon retourne une liste\n",
    "tokens_dev = tokeniser(data_dev.full_text, modele=glove, pad=False)\n",
    "if isinstance(tokens_dev,torch.Tensor):\n",
    "    print(tokens_dev.shape)\n",
    "else:\n",
    "    print(len(tokens_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cibles de validation\n",
    "cible_dev_real = torch.tensor(data_dev[\"true\"],dtype=torch.float32)\n",
    "# cible_dev_fake = torch.tensor(data_dev[\"false\"],dtype=torch.float32)\n",
    "# cible_dev_part = torch.tensor(data_dev[\"partially_false\"],dtype=torch.float32)\n",
    "# cible_dev_oth = torch.tensor(data_dev[\"other\"],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetoniser les donnees de test\n",
    "# Si pad=True, retourne un tenseur, sinon retourne une liste\n",
    "tokens_test = tokeniser(data_test.full_text,modele=glove,pad=False)\n",
    "if isinstance(tokens_test,torch.Tensor):\n",
    "    print(tokens_test.shape)\n",
    "else:\n",
    "    print(len(tokens_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cibles pour l'evaluation\n",
    "cible_train_dev_real = torch.cat((cible_train_real,cible_dev_real))\n",
    "cible_test_real = torch.tensor(data_test[\"true\"],dtype=torch.int)\n",
    "# cible_train_dev_fake = torch.cat((cible_train_fake,cible_dev_fake))\n",
    "# cible_test_fake = torch.tensor(data_test[\"false\"],dtype=torch.int)\n",
    "# cible_train_dev_part = torch.cat((cible_train_part,cible_dev_part))\n",
    "# cible_test_part = torch.tensor(data_test[\"partially_false\"],dtype=torch.int)\n",
    "# cible_train_dev_oth = torch.cat((cible_train_oth,cible_dev_oth))\n",
    "# cible_test_oth = torch.tensor(data_test[\"other\"],dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer datasets pour les modeles\n",
    "# Seulement pour sequences de longueur fixe i.e. pour tokens en tenseurs (n_phrases, max_mots, n_emb)\n",
    "max_mots = max(tokens_train.shape[1],tokens_dev.shape[1],tokens_test.shape[1])   # ATTENTION! Peut etre tres eleve, peut changer pour une valeur au choix\n",
    "dataset_train_real = FakeNewsDataset(ajuster_canaux(tokens_train,max_mots),cible_train_real)\n",
    "dataset_dev_real = FakeNewsDataset(ajuster_canaux(tokens_dev,max_mots),cible_dev_real)\n",
    "dataset_test_real = FakeNewsDataset(ajuster_canaux(tokens_test,max_mots),cible_test_real)\n",
    "# dataset_train_fake = FakeNewsDataset(ajuster_canaux(tokens_train,max_mots),cible_train_fake)\n",
    "# dataset_dev_fake = FakeNewsDataset(ajuster_canaux(tokens_dev,max_mots),cible_dev_fake)\n",
    "# dataset_test_fake = FakeNewsDataset(ajuster_canaux(tokens_test,max_mots),cible_test_fake)\n",
    "# dataset_train_part = FakeNewsDataset(ajuster_canaux(tokens_train,max_mots),cible_train_part)\n",
    "# dataset_dev_part = FakeNewsDataset(ajuster_canaux(tokens_dev,max_mots),cible_dev_part)\n",
    "# dataset_test_part = FakeNewsDataset(ajuster_canaux(tokens_test,max_mots),cible_test_part)\n",
    "# dataset_train_oth = FakeNewsDataset(ajuster_canaux(tokens_train,max_mots),cible_train_oth)\n",
    "# dataset_dev_oth = FakeNewsDataset(ajuster_canaux(tokens_dev,max_mots),cible_dev_oth)\n",
    "# dataset_test_oth = FakeNewsDataset(ajuster_canaux(tokens_test,max_mots),cible_test_oth)\n",
    "max_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IFT714",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
